{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building three RNN models for Sentiment Analysis, using three different architectures [Tanh, the traditional architecture, LSTM and GRU, the Gated Unit Architecture]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 10:00:02.781115: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-16 10:00:02.983047: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-16 10:00:02.983093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-16 10:00:02.984566: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-16 10:00:03.014436: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-16 10:00:03.015080: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-16 10:00:06.107461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers.embeddings import Embedding\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "\n",
    "units = 128\n",
    "embedding_size = 128\n",
    "vocab_size = 10000\n",
    "max_length = 256\n",
    "learning_rate = 0.01\n",
    "optimizer = 'rmsprop'\n",
    "test_size = 0.2\n",
    "batch_size = 32\n",
    "buffer_size = 128\n",
    "epochs = 10\n",
    "num_classes = 2\n",
    "data_path = 'IMDB_Dataset.csv'\n",
    "data_classes = {'negative': 0, 'positive': 1}\n",
    "data_name = 'review'\n",
    "label_name = 'sentiment'    \n",
    "\n",
    "# constants\n",
    "\n",
    "PADDING='post'\n",
    "TRUNC='post'\n",
    "OOV='<OOV>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model\n",
    "\n",
    "class GRU(tf.keras.layers.Layer):\n",
    "  '''\n",
    "    Arguments:\n",
    "      units (int): hidden dimension \n",
    "      inp_shape (int): Embedding dimension \n",
    "    Output:\n",
    "      h_t (Tensor): \n",
    "        Current hidden state\n",
    "        shape=(None, units) \n",
    "  '''\n",
    "\n",
    "  def __init__(self, units, inp_shape):\n",
    "    super(GRU, self).__init__()\n",
    "    self.units = units\n",
    "    self.inp_shape = inp_shape\n",
    "    self.W = self.add_weight(\"W\", shape=(3, self.units, self.inp_shape))\n",
    "    self.U = self.add_weight(\"U\", shape=(3, self.inp_shape, self.units))\n",
    "    \n",
    "  def call(self, pre_h, x):\n",
    "\n",
    "    # Update gate: Decide how much the unit updates its activation, or content\n",
    "    z_t = tf.nn.sigmoid(\n",
    "        tf.matmul(x, tf.transpose(self.W[0])) + tf.matmul(pre_h, tf.transpose(self.U[0])))\n",
    "\n",
    "    # Reset gate: Forget the previously state\n",
    "    r_t = tf.nn.sigmoid(\n",
    "        tf.matmul(x, tf.transpose(self.W[1])) + tf.matmul(pre_h, tf.transpose(self.U[1])))\n",
    "\n",
    "    # Current memory content\n",
    "    h_proposal = tf.nn.tanh(\n",
    "        tf.matmul(x, tf.transpose(self.W[2])) + tf.matmul(tf.multiply(r_t, pre_h), tf.transpose(self.U[2])))\n",
    "\n",
    "    # Current hidden state\n",
    "    h_t = tf.multiply((1 - z_t), pre_h) + tf.multiply(z_t, h_proposal)\n",
    "    \n",
    "    return h_t\n",
    "\n",
    "# Define GRU model\n",
    "class GRU_RNN(tf.keras.Model):\n",
    "  \"\"\"\n",
    "    Using GRU cell and Dense layers for training model\n",
    "  \"\"\"\n",
    "  def __init__(self, units, embedding_size, vocab_size, input_length, num_class):\n",
    "    super(GRU_RNN, self).__init__()\n",
    "    self.input_length = input_length\n",
    "    self.units = units\n",
    "    self.num_class = num_class\n",
    "\n",
    "    # Embedding\n",
    "    self.embedding = tf.keras.layers.Embedding(\n",
    "      vocab_size,\n",
    "      embedding_size,\n",
    "      input_length=input_length\n",
    "    )\n",
    "\n",
    "    # Using gru cell\n",
    "    self.model = GRU(units, embedding_size)\n",
    "\n",
    "    # Pass each hidden state through Rnn basic\n",
    "    self.classification_layer = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(32, input_shape=(units,), activation=\"relu\"),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(num_class, activation='softmax')\n",
    "    ])\n",
    "\n",
    "  def call(self, sentence):\n",
    "    \"\"\"\n",
    "      parameters: sentence need to trained\n",
    "        type: Tensor\n",
    "        shape: ( batch_size, input_length)\n",
    "\n",
    "      return: Predition by lastest of model\n",
    "        type: Tensor\n",
    "        shape: (batch_size,1)\n",
    "    \"\"\"\n",
    "    batch_size = tf.shape(sentence)[0]\n",
    "\n",
    "    # Initial hidden_state\n",
    "    pre_h = tf.zeros([batch_size, self.units])\n",
    "\n",
    "    # embedded_sentence: (batch_size, input_length, embedding_size)\n",
    "    embedded_sentence = self.embedding(sentence)\n",
    "    \n",
    "    for i in range(self.input_length):\n",
    "      word = embedded_sentence[:, i, :]\n",
    "      pre_h = self.model(pre_h, word)\n",
    "    \n",
    "    h = pre_h\n",
    "\n",
    "    # Predition by lastest hidden_state\n",
    "    output = self.classification_layer(h)\n",
    "    # print(\"===output_layer===\", output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "\n",
    "class LSTM(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Scratch LSTM with the equations by modifying the original LSTM tensorflow model\n",
    "    \"\"\"\n",
    "    def __init__(self, units, inp_shape):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.units = units\n",
    "        self.inp_shape = inp_shape\n",
    "        self.W = self.add_weight(\"W\", shape=(4, self.units, self.inp_shape))\n",
    "        self.U = self.add_weight(\"U\", shape=(4, self.units, self.units))\n",
    "\n",
    "    def call(self, pre_layer, x):\n",
    "        pre_h, pre_c = tf.unstack(pre_layer)\n",
    "\n",
    "        # Control the input values :  Input Gate:\n",
    "        i_t = tf.nn.sigmoid(tf.matmul(x, tf.transpose(self.W[0])) + tf.matmul(pre_h, tf.transpose(self.U[0])))\n",
    "\n",
    "        # Control the numbers of data need to keep: Forget Gate\n",
    "        f_t = tf.nn.sigmoid(tf.matmul(x, tf.transpose(self.W[1])) + tf.matmul(pre_h, tf.transpose(self.U[1])))\n",
    "\n",
    "        # Control the numbers of data in output: Output Gate\n",
    "        o_t = tf.nn.sigmoid(tf.matmul(x, tf.transpose(self.W[2])) + tf.matmul(pre_h, tf.transpose(self.U[2])))\n",
    "\n",
    "        # New memory for new information\n",
    "        n_c_t = tf.nn.tanh(tf.matmul(x, tf.transpose(self.W[3])) + tf.matmul(pre_h, tf.transpose(self.U[3])))\n",
    "\n",
    "        # Combination between storing information and new information\n",
    "        c = tf.multiply(f_t, pre_c) + tf.multiply(i_t, n_c_t)\n",
    "\n",
    "        # How information are allowed to be output of cell\n",
    "        h = tf.multiply(o_t, tf.nn.tanh(c))\n",
    "\n",
    "        return tf.stack([h, c])\n",
    "         \n",
    "\n",
    "class LSTM_RNN(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Using LSTM cell and Dense layers for training model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, embedding_size, vocab_size, input_length, num_class):\n",
    "        super(LSTM_RNN,self).__init__()\n",
    "        self.input_length = input_length\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_size,\n",
    "            input_length=input_length\n",
    "        )\n",
    "\n",
    "        self.LSTM = LSTM(units, embedding_size)\n",
    "\n",
    "        self.classification_layer = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(32, input_shape=(units,), activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(num_class, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, sentence):\n",
    "        \"\"\"\n",
    "        param: sentence need to trained\n",
    "            type: Tensor\n",
    "            shape: ( batch_size, input_length)\n",
    "\n",
    "        return: Output predicted by the model\n",
    "            type: Tensor\n",
    "            shape: (batch_size,1)\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(sentence)[0]\n",
    "\n",
    "        # create hidden_state and context_state\n",
    "        pre_layer = tf.stack([\n",
    "            tf.zeros([batch_size, self.units]),\n",
    "            tf.zeros([batch_size, self.units])\n",
    "        ])\n",
    "\n",
    "        # Put sentence into Embedding\n",
    "        embedded_sentence = self.embedding(sentence)\n",
    "\n",
    "        # Use LSTM with every single word in sentence\n",
    "        for i in range (self.input_length):\n",
    "            word = embedded_sentence[:, i, :]\n",
    "            pre_layer = self.LSTM(pre_layer, word)\n",
    "\n",
    "        # Take the last hidden _state\n",
    "        h, _ = tf.unstack(pre_layer)\n",
    "\n",
    "        # Using last hidden_state for for predicting or other processing\n",
    "        return self.classification_layer(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tanh Model (Traditional RNN)\n",
    "\n",
    "class Tanh(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "        Using traditional RNN but the bounded function is a tanh function\n",
    "    \"\"\"\n",
    "    def __init__(self, units, inp_shape):\n",
    "        super(Tanh,self).__init__()\n",
    "        self.units = units\n",
    "        self.inp_shape = inp_shape\n",
    "        self.W = self.add_weight(\"W\", shape=(1, self.units, self.inp_shape))\n",
    "        self.U = self.add_weight(\"U\", shape=(1, self.units, self.units))\n",
    "\n",
    "    def call(self, pre_layer, x):\n",
    "        # pre_h, pre_c = tf.unstack(pre_layer)\n",
    "        h = tf.nn.tanh(tf.matmul(x, tf.transpose(self.W[0])) + tf.matmul(pre_layer, tf.transpose(self.U[0])))\n",
    "        return h\n",
    "    \n",
    "class Tanh_RNN(tf.keras.Model):\n",
    "    \"\"\"\n",
    "        Using Tanh and Dense layers for training model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, units, embedding_size, vocab_size, input_length, num_class):\n",
    "        super(Tanh_RNN,self).__init__()\n",
    "        self.input_length = input_length\n",
    "        self.units = units\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_size,\n",
    "            input_length=input_length\n",
    "        )\n",
    "\n",
    "        self.model = Tanh(units, embedding_size)\n",
    "\n",
    "        self.classification_layer = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Dense(32, input_shape=(units,), activation=\"relu\"),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(num_class, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    def call(self, sentence):\n",
    "        \"\"\"\n",
    "        param: sentence need to trained\n",
    "            type: Tensor\n",
    "            shape: ( batch_size, input_length)\n",
    "\n",
    "        return: Output predicted by the model\n",
    "            type: Tensor\n",
    "            shape: (batch_size,1)\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(sentence)[0]\n",
    "\n",
    "        # create hidden_state and context_state\n",
    "        pre_layer = tf.zeros([batch_size, self.units])\n",
    "\n",
    "        # Put sentence into Embedding\n",
    "        embedded_sentence = self.embedding(sentence)\n",
    "\n",
    "        # Use Tanh with every single word in sentence\n",
    "        for i in range(self.input_length):\n",
    "            word = embedded_sentence[:, i, :]\n",
    "            pre_layer = self.model(pre_layer, word)\n",
    "\n",
    "        # Using last hidden_state for for predicting or other processing\n",
    "        return self.classification_layer(pre_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Class\n",
    "\n",
    "class Dataset:\n",
    "  def __init__(self, data_path, vocab_size, data_classes):\n",
    "    self.data_path = data_path\n",
    "    self.vocab_size = vocab_size\n",
    "    self.data_classes = data_classes\n",
    "    self.sentences_tokenizer = None\n",
    "    self.label_dict = None\n",
    "\n",
    "  def labels_encode(self, labels, data_classes):\n",
    "    '''Encode labels to categorical'''\n",
    "    labels.replace(data_classes, inplace=True)\n",
    "\n",
    "    labels_target = labels.values\n",
    "    labels_target = tf.keras.utils.to_categorical(labels_target)\n",
    "\n",
    "    return labels_target\n",
    "  \n",
    "  def removeHTML(self, text):\n",
    "    '''Remove html tags from a string'''\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "  \n",
    "  def removePunc(self, text):\n",
    "        #Remove punction in a texts\n",
    "        return re.sub(r'[^\\w\\s]','', text)\n",
    "  \n",
    "  def removeURLs(self, text):\n",
    "        #Remove url link in texts\n",
    "        return re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "\n",
    "  def removeEmoji(self, data):\n",
    "        #Each emoji icon has their unique code\n",
    "        #Gather all emoji icon code and remove it in texts\n",
    "        cleanr= re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  \n",
    "                           u\"\\U0001F300-\\U0001F5FF\"\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "        return re.sub(cleanr, '',data)\n",
    "\n",
    "  def sentence_cleaning(self, sentence):\n",
    "    '''Cleaning text'''\n",
    "    out_sentence = []\n",
    "    for line in tqdm(sentence):\n",
    "      line = self.removeHTML(line)\n",
    "      line = self.removePunc(line)\n",
    "      line = self.removeURLs(line)\n",
    "      line = self.removeEmoji(line)\n",
    "      text = re.sub(\"[^a-zA-Z]\", \" \", line)\n",
    "      word = word_tokenize(text.lower())\n",
    "\n",
    "      lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "      lemm_word = [lemmatizer.lemmatize(i) for i in word]\n",
    "\n",
    "      out_sentence.append(lemm_word)\n",
    "    return (out_sentence)\n",
    "\n",
    "  def data_processing(self, sentences, labels):\n",
    "    '''Preprocessing both text and labels'''\n",
    "    print(\"|--data_processing ...\")\n",
    "    sentences = self.sentence_cleaning(sentences)\n",
    "    labels = self.labels_encode(labels, data_classes=self.data_classes)\n",
    "    \n",
    "    return sentences, labels\n",
    "\n",
    "  def build_tokenizer(self, sentences, vocab_size, char_level=False):\n",
    "    print(\"|--build_tokenizer ...\")\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words= vocab_size, oov_token=OOV, char_level=char_level)\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "  def tokenize(self, tokenizer, sentences, max_length):\n",
    "    print(\"|--tokenize ...\")\n",
    "    sentences = tokenizer.texts_to_sequences(sentences)\n",
    "    sentences = tf.keras.preprocessing.sequence.pad_sequences(sentences, maxlen=max_length,\n",
    "                                                              padding=PADDING, truncating=TRUNC)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "  def load_dataset(self, max_length, data_name, label_name):\n",
    "    print(\" \")\n",
    "    print(\"Load dataset ... \")\n",
    "    datastore = pd.read_csv(self.data_path)\n",
    "    print(datastore.head())\n",
    "    sentences = datastore[data_name]\n",
    "    labels = datastore[label_name]\n",
    "\n",
    "    # Cleaning\n",
    "    sentences, labels = self.data_processing(sentences, labels)\n",
    "        \n",
    "    # Tokenizing\n",
    "    self.sentences_tokenizer = self.build_tokenizer(sentences, self.vocab_size)\n",
    "    tensor = self.tokenize(\n",
    "        self.sentences_tokenizer, sentences, max_length)\n",
    "\n",
    "    print(\"Done! Next to ... \")\n",
    "    print(\" \")\n",
    "\n",
    "    # Saving label dict\n",
    "    with open('label.json', 'w') as f:\n",
    "        json.dump(self.label_dict, f)\n",
    "        \n",
    "    return tensor, labels\n",
    "                                                                  \n",
    "  def build_dataset(self, max_length=128, test_size=0.2, buffer_size=128, batch_size=128, data_name='review', label_name='sentiment'):\n",
    "    sentences, labels = self.load_dataset(\n",
    "        max_length, data_name, label_name)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        sentences, labels, test_size=test_size, stratify=labels, random_state=42)\n",
    "\n",
    "    # Convert to tensor\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(\n",
    "        X_train, dtype=tf.int64), tf.convert_to_tensor(y_train, dtype=tf.int64)))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((tf.convert_to_tensor(\n",
    "        X_val, dtype=tf.int64), tf.convert_to_tensor(y_val, dtype=tf.int64)))\n",
    "    val_dataset = val_dataset.shuffle(buffer_size).batch(batch_size)\n",
    "   \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Load dataset ... \n",
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "|--data_processing ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [01:16<00:00, 649.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--build_tokenizer ...\n",
      "|--tokenize ...\n",
      "Done! Next to ... \n",
      " \n"
     ]
    }
   ],
   "source": [
    "# prepare the data\n",
    "\n",
    "dataset = Dataset(data_path, vocab_size, data_classes=data_classes)\n",
    "\n",
    "train_ds, val_ds = dataset.build_dataset(max_length, test_size, buffer_size, batch_size, data_name, label_name)\n",
    "sentences_tokenizer = dataset.sentences_tokenizer\n",
    "sentences_tokenizer_size = len(sentences_tokenizer.word_counts) + 1\n",
    "    \n",
    "# Initializing variables\n",
    "input_length = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the models\n",
    "\n",
    "gru_model = GRU_RNN(units, embedding_size, sentences_tokenizer_size, input_length, num_classes)\n",
    "lstm_model = LSTM_RNN(units, embedding_size, sentences_tokenizer_size, input_length, num_classes)\n",
    "tanh_model = Tanh_RNN(units, embedding_size, sentences_tokenizer_size, input_length, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the loss function and optimizer\n",
    "\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(name='categrical_crossentropy')\n",
    "optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate, name='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile optimizer and loss function into model\n",
    "\n",
    "metrics = ['accuracy', 'mse']\n",
    "\n",
    "gru_model.compile(optimizer=optimizer, loss=loss_object, metrics=metrics)\n",
    "lstm_model.compile(optimizer=optimizer, loss=loss_object, metrics=metrics)\n",
    "tanh_model.compile(optimizer=optimizer, loss=loss_object, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "# Callbacks: Early stopping\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2)\n",
    "\n",
    "checkpoint_path = \"cp.ckpt\"\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, verbose=1, save_weights_only=True, period=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 0s - loss: 0.7812 - accuracy: 0.5000 - mse: 0.2819"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Define the TensorBoard callback\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "\n",
    "# Train the models with reduced steps_per_epoch\n",
    "gru_history = gru_model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[early_stopping, checkpoint, tensorboard_callback], verbose=1, steps_per_epoch=10, batch_size=batch_size)\n",
    "lstm_history = lstm_model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[early_stopping, checkpoint, tensorboard_callback], verbose=1, steps_per_epoch=10, batch_size=batch_size)\n",
    "tanh_history = tanh_model.fit(train_ds, epochs=epochs, validation_data=val_ds, callbacks=[early_stopping, checkpoint, tensorboard_callback], verbose=1, steps_per_epoch=10, batch_size=batch_size)\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(gru_history.history['loss'], label='GRU Training Loss')\n",
    "plt.plot(gru_history.history['val_loss'], label='GRU Validation Loss')\n",
    "plt.plot(lstm_history.history['loss'], label='LSTM Training Loss')\n",
    "plt.plot(lstm_history.history['val_loss'], label='LSTM Validation Loss')\n",
    "plt.plot(tanh_history.history['loss'], label='Tanh Training Loss')\n",
    "plt.plot(tanh_history.history['val_loss'], label='Tanh Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot accuracies\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(gru_history.history['accuracy'], label='GRU Training Accuracy')\n",
    "plt.plot(gru_history.history['val_accuracy'], label='GRU Validation Accuracy')\n",
    "plt.plot(lstm_history.history['accuracy'], label='LSTM Training Accuracy')\n",
    "plt.plot(lstm_history.history['val_accuracy'], label='LSTM Validation Accuracy')\n",
    "plt.plot(tanh_history.history['accuracy'], label='Tanh Training Accuracy')\n",
    "plt.plot(tanh_history.history['val_accuracy'], label='Tanh Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
